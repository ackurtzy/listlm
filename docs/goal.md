# goal.md

## System Goal
Create a terminal-based, Python-only workflow that uses the **OpenAI API for every LLM step** to generate a multi-purpose spreadsheet (CSV) from live web data. The system must plan searches, let the user approve or adjust them, execute the searches, learn from what worked, and retry until it meets the user’s minimum item requirement.

## Required Functionality

1. **User input (terminal)**
   - User provides:
     - A **text description** of what they are looking for.
     - A **minimum number of items** the final list must contain.
     - **Spreadsheet columns** they want in the final CSV (optional).
   - If the user does not provide columns, the system will ask an OpenAI model to define the structured output.

2. **Generate 50 searches (OpenAI API)**
   - The system calls the **OpenAI API** to produce **about 50 search tasks**.
   - This call must instruct the model to create **diverse strategies** for finding the items, including:
     - direct/general web search,
     - site/directory or aggregator pages,
     - local or news sources,
     - time-bounded searches (e.g. current year),
     - other high-yield approaches.
   - To improve reliability, this can be done in **batches** (e.g. 5 calls × 10 searches).
   - Each search task must have:
     - `id`
     - `query`
     - `strategy`
     - optional `rationale`

3. **Decide structured output (OpenAI API)**
   - If the user did **not** specify columns, the system calls the **OpenAI API** again.
   - This model decides the **schema** (the columns) for the final output.
   - The schema must be a simple ordered list of field names.
   - This satisfies: “Another AI decides how to make the structured output for the API.”

4. **Filter to the best 20 (OpenAI API)**
   - The system calls the **OpenAI API** a third time.
   - This model **filters** the 50 candidate searches down to the **most promising 20**.
   - The model returns **only the IDs** of the searches to keep.
   - If the filter returns nothing, the system must fall back to the unfiltered list.
   - This satisfies: “Second AI filters those to the most promising 20.”

5. **User approval / feedback (terminal)**
   - The system shows the 20 selected searches to the user in the terminal.
   - The user can:
     - **approve** the plan,
     - **drop** specific searches by ID,
     - **add** a new search (create a new `SearchTask` with strategy `web`),
     - **give feedback** (e.g. “focus on 2024–2025” or “include non-US sources”).
   - Feedback is stored and used in future regeneration steps.
   - This satisfies: “Asks the user to approve the plan or give feedback on the searches.”

6. **Perform all the searches (OpenAI API web search)**
   - For every **approved** search task, the system calls the **OpenAI API** again, this time using a **web-search-capable model**.
   - Each call should request a fixed number of results (e.g. 5–8), using the search task’s **strategy** to influence the query.
   - The result must be requested in **structured form** (title, url, snippet, source).
   - This satisfies: “Performs all the searches.”

7. **Normalize, attribute, and dedupe**
   - For every item returned by a search:
     - Map it to the **active schema** (user columns or AI-generated columns). Missing fields become empty strings.
     - Add `source_query_id` = the ID of the search that produced it.
     - Add `source_strategy` = the strategy of that search.
   - Insert into an **in-memory database** that:
     - deduplicates on `url`, or on `(title, source)` if `url` is missing,
     - stores all rows for export.
   - This satisfies: “From each search, the code adds the relevant output to a database.”

8. **Evaluate and retry if below minimum**
   - After all approved searches are executed, the system checks the count of stored rows.
   - If the count is **below** the user’s minimum:
     - The system builds a **performance report** that describes:
       - which search IDs got 0 results,
       - which search IDs got few results,
       - which strategies worked best,
       - the user feedback text.
     - The system calls the **OpenAI API** again with this report and the original description, asking for **new, better searches**.
     - Then it repeats: filter → user approval → execute.
   - This satisfies: “If fewer than the minimum number of items, it evaluates which searches were successful and why, then comes up with new searches based on this information.”

9. **Export final spreadsheet**
   - When the minimum item count is met, or when max retries are used up, the system:
     - combines all rows in the in-memory DB,
     - outputs a **CSV file** in the configured directory,
     - uses the chosen schema/columns as headers.
   - This satisfies: “A spreadsheet is generated by combining the structured outputs.”

## Conformance to Original Plan

The system **does** conform to the original plan:

- **User inputs**: description, minimum, columns → yes.
- **API model creates 50 possible searches with many strategies** → yes, done via OpenAI API in batched, schema-enforced calls.
- **Another AI decides structured output** → yes, conditional schema step via OpenAI API.
- **Second AI filters to 20** → yes, dedicated OpenAI API call that returns IDs.
- **User approval/feedback step** → yes, terminal interaction to approve, drop, add, or comment.
- **Performs all searches** → yes, one OpenAI web-search call per approved task.
- **If fewer than minimum, evaluate which succeeded and regenerate** → yes, performance-aware regeneration via OpenAI API.
- **Add relevant output to a database** → yes, normalized, attributed, deduped in-memory rows.
- **Generate spreadsheet by combining structured outputs** → yes, CSV export with user/AI schema.

## Key Properties

- All LLM/AI actions are through **OpenAI API only**.
- The workflow is **agentic**: plan → approve → act → reflect → refine.
- The output is always a **CSV** with predictable columns.
- The system is **search-strategy-aware** (different strategies per search).
- The system is **self-correcting** when it fails to reach the target item count.

